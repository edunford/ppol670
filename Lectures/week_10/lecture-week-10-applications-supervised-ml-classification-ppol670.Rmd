---
title: 
    <font class = "title-panel"> PPOL670 | Introduction to Data Science for Public Policy  </font>
  <font size=6, face="bold"> Week 10 </font> 
   <br>
  <br>
  <font size=50, face="bold">Applications in Supervised Learning<br><br>_Classification_</font>
  <br>
  <br>
author: 
  <font class = "title-footer"> 
  &emsp;Prof. Eric Dunford &emsp;&#9670;&emsp; Georgetown University &emsp;&#9670;&emsp; McCourt School of Public Policy &emsp;&#9670;&emsp; eric.dunford@georgetown.edu</font>
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: "gu-theme.css"
    nature:
      highlightStyle: github
      beforeInit: "macros.js"
      countIncrementalSlides: False
      highlightLines: true
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
---

```{r setup, include=FALSE}
# Run for Interactive Slide Editing: 
# xaringan::inf_mr()
knitr::opts_chunk$set(echo = T,message=F,error=F,warning = F,cache=T)
require(tidyverse)
require(ggthemes)
```

layout: true

<div class="slide-footer"><span> 
PPOL670 | Introduction to Data Science for Public Policy

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;

Week 10 <!-- Week of the Footer Here -->

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;

Classification <!-- Title of the lecture here -->

</span></div> 

---
class: outline

# Outline for Today 

<br>

- **Refresh on Classification Problems**

- **Logistic Regression**

- **K Nearest Neighbors**

- **Classification Trees**

- **Support Vector Machines**


---

class: newsection

# Classification

---

<br>

.center[<img src = "Figures/seperability.gif", width = 500>]

---

### Decision Boundary 

.center[<img src = "Figures/decision-boundary.png", width = 600>]

---

### Classification Accuracy 


$$\hat{y}^{test}_i = \hat{f}(x^{test}_i)$$

.center[
| | $y^{test}_i$ |  $\hat{y}^{test}_i$ |  
|----|:------------:|:-------------------:|
| **_True Positive_** (TP) | 1 |  1 |  
| **_False Positive_** (FP) | 0 |  1 | 
| **_False Negative_** (FN) | 1 |  0 |  
| **_True Negative_** (TN) | 0 |  0 |  
]

--

![:space 2]


.center[
**_Confusion Matrix_**

|                       |  $actual~(+)$ |  $actual~(-)$ |
|-----------------------|----------|----------|
| $Predicted~(+)$  |   True Positive (TP)       | False Positive (FP)          |
| $Predicted~(-)$  |   False Negative (FN)       |  True Negative (TN)         |

]


---

### Conditional Probability 

Given $y \in 0,1$ (two classes)

$$pr(y_i = 1 | X_i)$$

--

```{r,eval=F}
prediction <- machine_learning_model(training_data)
```
```{r,echo=F}
set.seed(123)
prediction <- round(runif(10,0,1),3)
```

--

```{r}
prediction
```

--

![:space 5]

```{r}
threshold <- .5
1*(prediction >= threshold)
```


---

### Conditional Probability 

Given $y \in 0,1$ (two classes)

$$pr(y_i = 1 | X_i)$$


```{r,eval=F}
prediction <- machine_learning_model(training_data)
```
```{r,echo=F}
set.seed(123)
prediction <- round(runif(10,0,1),3)
```


```{r}
prediction
```

![:space 5]

```{r,highlight=T}
threshold <- .9 # Higher threshold, more false negatives  #<<
1*(prediction >= threshold)
```

---

### Conditional Probability 

Given $y \in 0,1$ (two classes)

$$pr(y_i = 1 | X_i)$$


```{r,eval=F}
prediction <- machine_learning_model(training_data)
```
```{r,echo=F}
set.seed(123)
prediction <- round(runif(10,0,1),3)
```


```{r}
prediction
```

![:space 5]

```{r,highlight=T}
threshold <- .1 # lower threshold, more false positives  #<<
1*(prediction >= threshold)
```


---

### ROC and AUC

.center[<img src = "Figures/roc-plot.png", width = 600>]

---

### Other Classification Metrics 

![:space 15]

| Metric | Calculation |  Description |
|:---:|:-----:| -----|
| Accuracy | $\frac{TP + TN}{TP+FP+TN+FN}$ | In total, how accurate is the model |
| Precision | $\frac{TP}{TP+FP}$ | Of the true positives classified, how many are actually positive | 
| Specificity | $\frac{ TN }{ TN + FP }$ | Of the actual true negatives, how many were correctly classified | 
| Recall/Sensitivity | $\frac{TP}{ TP + FN}$ | Of the actual true positives, how many were correctly classified |

---

class: newsection

<br>

# Logistic Regression

---

## The problem with linear regression 


```{r,echo=F,fig.align='center',fig.width=9,fig.height=6,dpi=300}
set.seed(123)
N = 250
x <- rnorm(N)
z =  pnorm(1 + 2*x)
y = rbinom(N,1,z)
D = tibble(y,x)

tt = element_text(family='serif',face='bold',size = 20)
ggplot(D,aes(x,y)) +
  geom_point(size=3,alpha=.5) +
  geom_smooth(method="lm",se=F,color="darkred",size=1.5) +
  theme_bw() +
  theme(axis.title = tt,
        axis.text = tt ) 
```

---

## Logistic Regression


```{r,echo=F,fig.align='center',fig.width=9,fig.height=6,dpi=300}
D$pr = predict(glm(y~x,data = D,family=binomial("probit")),type="response")

ggplot(D,aes(x,y)) +
  geom_point(size=3,alpha=.5) +
  geom_line(aes(x,pr),color="steelblue",size=2,alpha=.75) +
  theme_bw() +
  theme(axis.title = tt,
        axis.text = tt ) 

```


---

We need a function $F(\cdot)$ (which is known as a **_link function_**) that maps our linear combination of independent variables ( $X\beta$ ) onto a probability space (ranging from 0 to 1).

$$ F(X\beta) \mapsto [0,1]$$

.center[<img src="Figures/lin-to-pr-space.gif", width=400 >]

---

```{r,echo=F,fig.align="center",fig.width=10,fig.height=8}
bind_rows(
  tibble(x=seq(-5,5,.1),
         `CDF\nPr(y = 1)`=pnorm(seq(-5,5,.1)),
         `PDF\nDensity`=dnorm(seq(-5,5,.1)),type="Normal"),
  tibble(x=seq(-5,5,.1),
         `CDF\nPr(y = 1)`=plogis(seq(-5,5,.1),location = 0,scale=1),
         `PDF\nDensity` = dlogis(seq(-5,5,.1),location = 0,scale=1),
         type="Logistic")
) %>% 
  gather(key,val,-x,-type) %>% 
ggplot(aes(x,val,color=type)) +
  geom_line(size=2,alpha=.75) +
  labs(y='',color="",pch="",x= latex2exp::TeX("$\\beta_0 + \\beta_1 x_i$")) +
  scale_color_manual(values=c("#410B9E", "#FF791F"))+
  theme_bw() +
  theme(axis.title = tt,
        axis.text = tt,
        legend.text = tt,
        legend.position = "top",
        strip.text = tt,strip.background = element_blank()) +
  facet_wrap(~key,scales="free",strip.position = "right",ncol=1)
```

---

```{r,echo=F,cache=F}
set.seed(123)
N = 600
x1 <- rnorm(N)           
x2 <- rnorm(N)           
y_star <- .5*x1 + .6*x2

# convert to a probability space
pr <- exp(y_star)/(1+exp(y_star))

# Drop the probability into a bernoulli dist. (0 or 1)
y <- rbinom(N, size=1, prob = pr) 

# Gather as dataset
dat <- tibble(y,x1,x2)
training_data = dat[1:500,]
test_data = dat[500:600,]
```

```{r}
head(training_data,3)
```


```{r,fig.align="center",fig.width=10,fig.height=3.75,cache=F}
pairs(training_data,col="steelblue")
```

---

### Estimate

```{r,cache=F}
#We can easily estimate these models in base R
mod = glm(y ~ x1 + x2, data=training_data,
          family=binomial(link = "logit"))
```

![:space 5]

### Predict

```{r,cache=F}
preds = predict(mod, test_data,type = "response")
head(preds)
```

---

### Estimate

```{r,cache=F}
#We can easily estimate these models in base R
mod = glm(y ~ x1 + x2, data=training_data,
          family=binomial(link = "logit"))
```

![:space 5]

### Predict
```{r,cache=F}
preds = predict(mod, test_data,type = "response")
table(preds > .5, test_data$y)
```

---

class: newsection

<br>

# $K$-Nearest Neighbors

---


```{r,echo=F,fig.align="center",fig.width=10,fig.height=8}
# Create Data
set.seed(1234)
N = 50
x1 = runif(N)
x2 = runif(N)
y = rbinom(N,1,.5)
D = tibble(x1,x2,y)
D$x1_star = .63
D$x2_star = .37
D <- 
  D %>% 
  mutate(distance_x1 = sqrt((x1 - x1_star)^2),
         distance_x2 = sqrt((x2 - x2_star)^2),
         distance = (distance_x1 + distance_x2)/2 )

new_size = 6

D %>% 
  ggplot(aes(x1,x2,color=factor(y))) +
  geom_point(size=3) +
  scale_color_manual(values=c("steelblue","darkred")) +
  theme_minimal() +
  theme(text = element_text(size=24),
        legend.position = "none",
        legend.text = element_text(size=20),
        title = element_text(hjust=.5)) 
```


---

```{r,echo=F,fig.align="center",fig.width=10,fig.height=8}
D %>% 
  ggplot(aes(x1,x2,color=factor(y))) +
  geom_point(size=3) +
  scale_color_manual(values=c("steelblue","darkred")) +
  geom_point(x=.63,y=.37,color="black",pch=18,size=new_size) +
  theme_minimal() +
  theme(text = element_text(size=24),
        legend.position = "none",
        legend.text = element_text(size=20),
        title = element_text(hjust=.5)) 
```

---

```{r,echo=F,fig.align="center",fig.width=10,fig.height=8}
D %>% 
  ggplot(aes(x1,x2,color=factor(y))) +
  geom_point(size=3) +
  scale_color_manual(values=c("steelblue","darkred")) +
  geom_point(x=.63,y=.37,color="black",pch=18,size=new_size) +
  geom_segment(aes(x=x1_star,xend=x1,y=x2_star,yend=x2),alpha=.2,size=1) +
  geom_point(x=.63,y=.37,color="black",pch=18,size=new_size) +
  theme_minimal() +
  theme(text = element_text(size=24),
        legend.position = "none",
        legend.text = element_text(size=20),
        title = element_text(hjust=.5)) 
```

---

```{r,echo=F,fig.align="center",fig.width=10,fig.height=8}
D %>% 
  ggplot(aes(x1,x2,color=factor(y))) +
  geom_point(size=3) +
  scale_color_manual(values=c("steelblue","darkred")) +
  geom_point(x=.63,y=.37,color="black",pch=18,size=new_size) +
  geom_segment(aes(x=x1_star,xend=x1,y=x2_star,yend=x2),alpha=.2,size=1) +
  geom_point(x=.63,y=.37,color="black",pch=18,size=new_size) +
  gghighlight::gghighlight(distance <.07  ) +
  theme_minimal() +
  theme(text = element_text(size=24),
        legend.position = "none",
        legend.text = element_text(size=20),
        title = element_text(hjust=.5)) 
```


---

```{r,echo=F,fig.align="center",fig.width=10,fig.height=8}
D %>% 
  ggplot(aes(x1,x2,color=factor(y))) +
  geom_point(size=3) +
  scale_color_manual(values=c("steelblue","darkred")) +
  geom_point(x=.63,y=.37,color="black",pch=18,size=new_size) +
  geom_segment(aes(x=x1_star,xend=x1,y=x2_star,yend=x2),alpha=.2,size=1) +
  geom_point(x=.63,y=.37,color="black",pch=18,size=new_size) +
  gghighlight::gghighlight(distance <.07  ) +
  geom_point(x=.63,y=.37,color="darkred",pch=18,size=new_size + .5) +
  theme_minimal() +
  theme(text = element_text(size=24),
        legend.position = "none",
        legend.text = element_text(size=20),
        title = element_text(hjust=.5)) 
```

---

### KNN

<br>

.center[<img src =  "Figures/knn.png", width = 700> ]

---

### KNN

- Non-parametric method that treats inputs as coordinate sets

- Classifies by distance of the new entry (test data) to existing entries (training data). 

--

- Distance can be conceptualized in a number ways. Euclidean distance is common:

$$distance = \sqrt{(x_{ij} - x_{0j})^2}$$

--

- Classification occurs as a "_majority vote_"

$$Pr(y_{ik} = j~|~X = x_{ik}) = \frac{\sum^K_{k=1} I(y_{ik} =j )}{K}$$

--

- Poor performance well in high dimensions


---

### $k$ is a tuning parameter

<br>

.center[<img src =  "Figures/low-high-k.png", width = 700> ]


---

### $k$ is a tuning parameter

.center[<img src =  "Figures/knn-overfitting.png", width = 700> ]


---

class: newsection

# Classification Trees


---

## Refresh on Regression Trees 

- The goal is to Ô¨Ånd boxes that minimize the predictive error in our training data. 

- **_Recursive Binary Splitting_**

  - **Top-down**: start with one region and break from there.

  - **Greedy**: best split is made at each step (best split given the other splits that have been made)

- **_Tree Depth_**
  
  - Shallow trees (a few splits) can result in underfitting. 

  - Deep trees (many splits) can result in overfitting

---

### Classification Trees

<br>

- Categorical rather than continuous outcome

- Similar process to a regression tree.

- Predict most commonly occurring class of training observations in the region to which it belongs.

- Use the **_Gini Index_** as a measurement of error

$$G = \sum^K_{k=1} \hat{p}_{mk} (1-\hat{p}_{mk})$$

- Gini index gets small if all $\hat{p}_{mk}$ are close to zero or one ("node purity")

---

<br><br><br><br>

.center[
<img src =  "Figures/classification-tree-01.png", width = 1000> 
]


---

### Reminder: Regression vs. Trees

.center[
<img src =  "Figures/reg-v-trees.png", width = 600> 
]

---

class: newsection

# Support Vector Machines

---

### Let's Build a Wall

```{r,echo=F,fig.align="center",fig.height=7,fig.width=10}
set.seed(123)
n = 1e4
x1 = runif(n)
x2 = runif(n)
y = 0+1*x1 >= x2
D = tibble(y,x1,x2)
D %>% 
  ggplot(aes(x1,x2,color=as.factor(y))) +
  geom_point(show.legend = F,alpha=.3,size=3) +
  geom_abline(intercept = 0,slope=1,size=3,color="grey20") +
  ggthemes::theme_fivethirtyeight() +  
  ggthemes::scale_color_gdocs()  
```


---

### Separating Hyperplane

$$y_i(\beta_0 + x_{1i}\beta_1 + \dots + x_{pi}\beta_p) > 0, \text{ if }y_i = 1$$
$$y_i(\beta_0 + x_{1i}\beta_1 + \dots + x_{pi}\beta_p) < 0, \text{ if }y_i = -1$$

.center[
<img src =  "Figures/hyperplane.png", width = 1000> 
]

---

### Maximal Margin Hyperplane

.center[
<img src =  "Figures/mm_hyperplane.png", width = 550> 
]

---

### Non-separable

.center[
<img src =  "Figures/nonseparable.png", width = 550> 
]

---

### Support Vector Classifier 

![:space 10]
.center[
<img src =  "Figures/svc_01.png", width = 1000> 
]

---

### Support Vector Classifier 

**_Aim_**: maximize the margin that separates most of the training observations but misclassifies only a few observations. 

$$max_{\beta, \epsilon}~M~\text{ subject to } \sum_{j=1}^p \beta_j^2 = 1$$

$$y_i(\beta_0 + x_{1i}\beta_1 + \dots + x_{pi}\beta_p) \ge M (1-\epsilon_i)$$

$$\epsilon \ge 0, \sum_{i=1}^n \epsilon_i \le C$$
Where $C$ is a nonnegative tuning parameter. 

- $C$ dictates how many individuals observations can be on the wrong side of the margin. 

- $C$ &rarr; 0 high bias; $C$ &rarr; 1 high variability

---

### Tuning $C$

.center[
<img src =  "Figures/tune_c.png", width = 530> 
]

---

### Dealing with Non-Linear Boundaries 

![:space 5]

.center[
<img src =  "Figures/nonlinear_boundary.png", width = 1000> 
]

---

### Support Vector Machine

Use a (polynomial, radial) _kernel_ to generate a decision boundary.

<br>

.center[
<img src =  "Figures/svm.png", width = 1000> 
]
