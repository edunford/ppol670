<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title> PPOL670 | Introduction to Data Science for Public Policy   Week 10      Applications in Supervised Learning  Classification</title>
    <meta charset="utf-8" />
    <meta name="author" content="  Prof. Eric Dunford  ◆  Georgetown University  ◆  McCourt School of Public Policy  ◆  eric.dunford@georgetown.edu" />
    <link rel="stylesheet" href="gu-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <font class = "title-panel"> PPOL670 | Introduction to Data Science for Public Policy </font> <font size=6, face="bold"> Week 10 </font> <br> <br> <font size=50, face="bold">Applications in Supervised Learning<br><br><em>Classification</em></font> <br> <br>
### <font class = "title-footer">  Prof. Eric Dunford  ◆  Georgetown University  ◆  McCourt School of Public Policy  ◆  <a href="mailto:eric.dunford@georgetown.edu" class="email">eric.dunford@georgetown.edu</a></font>

---




layout: true

&lt;div class="slide-footer"&gt;&lt;span&gt; 
PPOL670 | Introduction to Data Science for Public Policy

&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;

Week 10 &lt;!-- Week of the Footer Here --&gt;

&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;

Classification &lt;!-- Title of the lecture here --&gt;

&lt;/span&gt;&lt;/div&gt; 

---
class: outline

# Outline for Today 

&lt;br&gt;

- **Refresh on Classification Problems**

- **Logistic Regression**

- **K Nearest Neighbors**

- **Classification Trees**

- **Support Vector Machines**


---

class: newsection

# Classification

---

&lt;br&gt;

.center[&lt;img src = "Figures/seperability.gif", width = 500&gt;]

---

### Decision Boundary 

.center[&lt;img src = "Figures/decision-boundary.png", width = 600&gt;]

---

### Conditional Probability 

Given `\(y \in 0,1\)` (two classes)

`$$pr(y_i = 1 | X = \vec{x_i})$$`

--

&lt;br&gt;


`$$\hat{f}(\vec{x_i}) \in [0,1]$$` 

where `\(\hat{f}(\cdot)\)` is an approximation for the true data generating process `\(f(\cdot)\)` and `\(\vec{x_i}\)` is a vector of variables for observation `\(i\)`.

--

&lt;br&gt; 

`$$\hat{y_i} =\begin{cases} 1~~\text{if}~~\hat{f}(\vec{x_i})  &gt; .5 \\ 0~~\text{if}~~\hat{f}(\vec{x_i})  \le .5 \end{cases}$$`

---

&lt;br&gt;&lt;br&gt;&lt;br&gt;

`$$\hat{y_i} =\begin{cases} 1~~\text{if}~~\hat{f}(\vec{x_i})  &gt; .5 \\ 0~~\text{if}~~\hat{f}(\vec{x_i})  \le .5 \end{cases}$$`

&lt;br&gt;

.center[.5 is an arbitrary threshold ( `\(\tau\)` ) that we can adjust. 

larger `\(\tau\)` means more false negatives

smaller `\(\tau\)` means more false positives
 ]


---

### Accuracy 


`$$\hat{y}^{test}_i = \hat{f}(x^{test}_i)$$`

.center[
| | `\(y^{test}_i\)` |  `\(\hat{y}^{test}_i\)` |  
|----|:------------:|:-------------------:|
| **True Positive** | 1 |  1 |  
| **False Positive** | 0 |  1 | 
| **False Negative** | 1 |  0 |  
| **True Negative** | 0 |  0 |  
]

--

&lt;br&gt;

$$ \text{average test error} = \frac{\sum^N_{i=1} I(y^{test}_i \ne \hat{y}^{test}_i)}{N}$$

---

### Accuracy

.center[
|                       |  `\(actual~(+)\)` |  `\(actual~(-)\)` |
|-----------------------|----------|----------|
| `\(Predicted~(+)\)`  |   True Positive (TP)       | False Positive (FP)          |
| `\(Predicted~(-)\)`  |   False Negative (FN)       |  True Negative (TN)         |

]

--

&lt;br&gt;

| Metric | Calculation |  Description |
|:---:|:-----:| -----|
| Accuracy | `\(\frac{TP + TN}{TP+FP+TN+FN}\)` | In total, how accurate is the model |
| Precision | `\(\frac{TP}{TP+FP}\)` | Of the true positives classified, how many are actually positive | 
| Specificity | `\(\frac{ TN }{ TN + FP }\)` | Of the actual true negatives, how many were correctly classified | 
| Recall/Sensitivity | `\(\frac{TP}{ TP + FN}\)` | Of the actual true positives, how many were correctly classified |

---

### ROC and AUC

.center[&lt;img src = "Figures/roc-plot.png", width = 600&gt;]

---

class: newsection

&lt;br&gt;

# Logistic Regression

---

## The problem with linear regression 

When we have a binary outcome (0/1), we can use OLS to estimate the relationship. This is known as a **linear probability model** (LPM).  

--

We start with the usual equation: 

`$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$`
Here `\(y \in [0,1]\)`
   
- The probability of "success": `\(pr(y_i = 1) = p_i\)`
- The probability of "failure": `\(pr(y_i = 0) = 1 - p_i\)` 

--

`\(y\)` follows a **Bernoulli** distribution

$$ E(y_i) = 0 \times (1-p_i) + 1 \times p_i = p_i$$
$$ E(y_i | x_i) = \beta_0 + \beta_1 x_i = p_i $$


---

## The problem with linear regression 

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

--

.center[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.690 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.034 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; x &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.328 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.037 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.756 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---

## The problem with linear regression 

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;



.center[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 69% &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.034 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; x &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 32.8% &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.037 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.756 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---

## The problem with linear regression 

- OLS may produce **non-sense predictions** (recall that OLS assumes continuous interval level data)

--

  + But we can **recode**
  
&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;


---

## The problem with linear regression 

- OLS may produce **non-sense predictions** (recall that OLS assumes continuous interval level data)

- Disturbances are &lt;u&gt;_not_&lt;/u&gt; normally distributed, they follow the Bernoulli distribution.

--

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;



---

## The problem with linear regression 

- OLS may produce **non-sense predictions** (recall that OLS assumes continuous interval level data)

- Disturbances are &lt;u&gt;_not_&lt;/u&gt; normally distributed, they follow the Bernoulli distribution.

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;


---


### Generalized Linear Model

- Let's specify a relationship between `\(x\)` and the probability of an event such that `\(pr(y_i = 1 | x)\)` is a function that ranges from `\(-\infty\)` to `\(\infty\)`. 


- First, we transform the outcome into the **odds**

`$$\frac{pr(y_i = 1 | x)}{pr(y_i = 0 | x)} = \frac{pr(y_i = 1 | x)}{1 - pr(y_i = 1 | x)}$$`

- **Odds** indicate how often something happens relative to how often it doesn't happen. 

- The **log of the odds** which ranges from `\(-\infty\)` to `\(\infty\)`.

`$$ln\begin{bmatrix}\frac{pr(y_i = 1 | x)}{1 - pr(y_i = 1 | x)}\end{bmatrix}  = X \beta$$`

---

We need a function `\(F(\cdot)\)` (which is known as a **link function**) that maps our linear combination of independent variables ( `\(X\beta\)` ) onto a probability space (ranging from 0 to 1).

$$ F(X\beta) \mapsto [0,1]$$

.center[&lt;img src="Figures/lin-to-pr-space.gif", width=400 &gt;]

---

![:space 10]

The **logistic (logit)** and **Normal (probit)** distributions are used frequently. 

![:space 10]

.center[
| Model | Distribution  | CDF | PDF  |
|---|---|-----|---|---|
| **_Probit_** | _Normal_ | `\(\Phi(\epsilon) = \int^\epsilon_{-\infty} \frac{1}{(\sqrt{2 \pi})} e^{-\frac{t^2}{2}} dt~~\)` | `\(\phi(\epsilon) = \frac{1}{(\sqrt{2 \pi})} e^{-\frac{\epsilon^2}{2}}\)` |
| **_Logit_** | _Logistic_ | `\(\Lambda(\epsilon) = \frac{e^\epsilon}{1 + e^\epsilon}\)` | `\(\lambda(\epsilon) = \frac{e^\epsilon}{(1 + e^\epsilon)^2}\)` |
]

---

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;


---



The outcome `\(y_i\)` follows a **Bernoulli distribution**.

`$$y_i \sim bernoulli(p_i)$$`
`$$y_i \sim p_i^{y_i}(1-p_i)^{1-y_i}$$`

--

&lt;br&gt;

We can plug in our linear combination of predictors into this probability distribution. All we need to make sure is that our predictors map to a probablity space (this is our "probability model") 

$$ \pi_i =logit(\beta_0 + \beta_1 x_i)  $$
&lt;br&gt;
`$$y_i \sim bernoulli(\pi_i)$$`

`$$y_i \sim \pi_i^{y_i}(1-\pi_i)^{1-y_i}$$`

---




```r
head(training_data,3)
```

```
## # A tibble: 3 x 3
##       y     x1      x2
##   &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;
## 1     0 -0.560  1.07  
## 2     1 -0.230 -0.0273
## 3     1  1.56  -0.0333
```



```r
pairs(training_data,col="steelblue")
```

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;

---

### Estimate


```r
mod = glm(y~x1 + x2,data=training_data,family=binomial(link = "probit"))
broom::tidy(mod)
```

```
## # A tibble: 3 x 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)  -0.0311    0.0579    -0.538 0.591       
## 2 x1            0.347     0.0619     5.60  0.0000000215
## 3 x2            0.235     0.0572     4.10  0.0000407
```


### Predict


```r
preds = predict(mod,test_data,type = "response")
head(preds)
```

```
##         1         2         3         4         5         6 
## 0.5409051 0.4902012 0.4256561 0.5417080 0.4602316 0.3027579
```

---

### Estimate


```r
mod = glm(y~x1 + x2,data=training_data,family=binomial(link = "probit"))
broom::tidy(mod)
```

```
## # A tibble: 3 x 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)  -0.0311    0.0579    -0.538 0.591       
## 2 x1            0.347     0.0619     5.60  0.0000000215
## 3 x2            0.235     0.0572     4.10  0.0000407
```


### Predict

```r
preds = predict(mod,test_data,type = "response")
table(preds&gt;.5,test_data$y)
```

```
##        
##          0  1
##   FALSE 40 19
##   TRUE  13 29
```

---

class: newsection

&lt;br&gt;

# `\(K\)`-Nearest Neighbors

---


&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;


---

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;


---

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;

---

### KNN

&lt;br&gt;

.center[&lt;img src =  "Figures/knn.png", width = 700&gt; ]

---

### KNN

- Non-parametric method that treats inputs as coordinate sets

- Classifies by distance of the new entry (test data) to existing entries (training data). 

--

- Distance can be conceptualized in a number ways. Euclidean distance is common:

`$$distance = \sqrt{(x_{ij} - x_{0j})^2}$$`

--

- Classification occurs as a "_majority vote_"

`$$Pr(y_{ik} = j~|~X = x_{ik}) = \frac{\sum^K_{k=1} I(y_{ik} =j )}{K}$$`

--

- Poor performance well in high dimensions


---

### `\(k\)` is a tuning parameter

&lt;br&gt;

.center[&lt;img src =  "Figures/low-high-k.png", width = 700&gt; ]


---

### `\(k\)` is a tuning parameter

.center[&lt;img src =  "Figures/knn-overfitting.png", width = 700&gt; ]


---

class: newsection

# Classification Trees


---

## Refresh on Regression Trees 

- The goal is to ﬁnd boxes that minimize the predictive error in our training data. 

- **_Recursive Binary Splitting_**

  - **Top-down**: start with one region and break from there.

  - **Greedy**: best split is made at each step (best split given the other splits that have been made)

- **_Tree Depth_**
  
  - Shallow trees (a few splits) can result in underfitting. 

  - Deep trees (many splits) can result in overfitting

---

### Classification Trees

&lt;br&gt;

- Categorical rather than continuous outcome

- Similar process to a regression tree.

- Predict most commonly occurring class of training observations in the region to which it belongs.

- Use the **_Gini Index_** as a measurement of error

`$$G = \sum^K_{k=1} \hat{p}_{mk} (1-\hat{p}_{mk})$$`

- Gini index gets small if all `\(\hat{p}_{mk}\)` are close to zero or one ("node purity")

---

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

.center[
&lt;img src =  "Figures/classification-tree-01.png", width = 1000&gt; 
]


---

### Reminder: Regression vs. Trees

.center[
&lt;img src =  "Figures/reg-v-trees.png", width = 600&gt; 
]

---

class: newsection

# Support Vector Machines

---

### Let's Build a Wall

&lt;img src="lecture-week-10-applications-supervised-ml-classification-ppol670_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;


---

### Separating Hyperplane

`$$y_i(\beta_0 + x_{1i}\beta_1 + \dots + x_{pi}\beta_p) &gt; 0, \text{ if }y_i = 1$$`
`$$y_i(\beta_0 + x_{1i}\beta_1 + \dots + x_{pi}\beta_p) &lt; 0, \text{ if }y_i = -1$$`

.center[
&lt;img src =  "Figures/hyperplane.png", width = 1000&gt; 
]

---

### Maximal Margin Hyperplane

.center[
&lt;img src =  "Figures/mm_hyperplane.png", width = 550&gt; 
]

---

### Non-separable

.center[
&lt;img src =  "Figures/nonseparable.png", width = 550&gt; 
]

---

### Support Vector Classifier 

![:space 10]
.center[
&lt;img src =  "Figures/svc_01.png", width = 1000&gt; 
]

---

### Support Vector Classifier 

**_Aim_**: maximize the margin that separates most of the training observations but misclassifies only a few observations. 

`$$max_{\beta, \epsilon}~M~\text{ subject to } \sum_{j=1}^p \beta_j^2 = 1$$`

`$$y_i(\beta_0 + x_{1i}\beta_1 + \dots + x_{pi}\beta_p) \ge M (1-\epsilon_i)$$`

`$$\epsilon \ge 0, \sum_{i=1}^n \epsilon_i \le C$$`
Where `\(C\)` is a nonnegative tuning parameter. 

- `\(C\)` dictates how many individuals observations can be on the wrong side of the margin. 

- `\(C\)` &amp;rarr; 0 high bias; `\(C\)` &amp;rarr; 1 high variability

---

### Tuning `\(C\)`

.center[
&lt;img src =  "Figures/tune_c.png", width = 530&gt; 
]

---

### Dealing with Non-Linear Boundaries 

![:space 5]

.center[
&lt;img src =  "Figures/nonlinear_boundary.png", width = 1000&gt; 
]

---

### Support Vector Machine

Use a (polynomial, radial) _kernel_ to generate a decision boundary.

&lt;br&gt;

.center[
&lt;img src =  "Figures/svm.png", width = 1000&gt; 
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
