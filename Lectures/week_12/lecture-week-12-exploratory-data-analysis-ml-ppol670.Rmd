---
title: 
    <font class = "title-panel"> PPOL670 | Introduction to Data Science for Public Policy  </font>
  <font size=6, face="bold"> Week 12 </font> 
  <br>
  <br>
  <font size=100, face="bold"> Exploratory Data Analysis </font>
author: 
  <font class = "title-footer"> 
  &emsp;Prof. Eric Dunford &emsp;&#9670;&emsp; Georgetown University &emsp;&#9670;&emsp; McCourt School of Public Policy &emsp;&#9670;&emsp; eric.dunford@georgetown.edu</font>
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: "gu-theme.css"
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      countIncrementalSlides: False
      highlightLines: true
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,message=F,error=F,warning = F,cache=T)
require(tidyverse)
require(recipes)
require(rsample)
require(caret)
doMC::registerDoMC()
```

layout: true

<div class="slide-footer"><span> 
PPOL670 | Introduction to Data Science for Public Policy

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;

Week 12 <!-- Week of the Footer Here -->

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;

Exploratory Data Analysis <!-- Title of the lecture here -->

</span></div> 

---
class: outline

# Outline for Today 

**Approaches to**

- **_Interpretable Machine Learning_**

  + _Variable Importance_
  + _Partial Dependencies_
  + _Individual Conditional Expectations_
  + _Global Surrogate Models_

- **_Data Exploration in the Social Sciences_**
  
  + Exploring data when you have _**limited priors**_
  +  Outline some **_steps_** to follow when exploring data. 


---

class: newsection
  
# Interpretable Machine Learning

---

## Model Interpretation

- Knowing a model is predictive is _necessary_ but rarely _sufficient_ to draw **_substantive insights_**. 

--

- In the social sciences, we are interested in understanding **_why_** certain features matter in an effort to detect potential **_interventions_**: if we change $X$ will we get a different outcome?

--

- Interpretability offers insights into the features the model **_relies on to make its prediction_**. 

--

- In addition, interpretability is a useful debugging tool for **_detecting bias_** in machine learning models. 

--

- Model needs to be a fairly **_good approximation of the data generating process_** (i.e. high predictive accuracy) for interpretation to matter



---

## Variable Importance

![:space 2]

- Variable/Feature importance is concerned with how much a given model **_relies on a set of variables/features to make accurate predictions_**. 

- If those variables/features were removed, the model should **_perform worse_** (i.e. diminished predictive capacity).

- Determining variable importance helps with **_variable selection_**.

  - What variables could we drop from the model (not contributing much information)? 
  
  - What variables should we make sure to always measure and use in the model?

---

## Variable Importance

Consider output from a simple multivariate variable OLS regression, what variables seem to matter most?

![:space 3]

```{r,echo=F}
set.seed(123)
N = 1000
x1 <- rnorm(N)
x2 <- rnorm(N)
x3 <- rnorm(N)
y <- 2*x1 + rnorm(N)
mod = lm(y ~ x1 + x2 + x3)
broom::tidy(mod)
```

--

![:space 3]

- `x1` is clearly both substantively and statistically significant. We should keep it in the model.

---


## Variable Importance

![:space 2]

- Some models offer a natural way of determining importance:

  - _Regression_: coefficient and test statistic size
  
  - _Trees_: split importance
  
![:space 2]

- But other models are more complicated (e.g. support vector machines, KNN, Neural Networks). We call these **_black box_** models because it's difficult to "peer inside" the model to understand how it works. 

![:space 2]

- We need ways of determining predictive performance that are **_model agnostic_** (i.e. doesn't depend on the type of model you use).

---

### Permutation Importance

- Permutation Importance offers a model agnostic way to determine variable importance.

- The idea: **_scramble the data_** one variable at a time and see if the predictive performance of the model _decreases_. 

--

- How it works:

  + **_Train_** a model
  
  + **_Permute_** (i.e. scramble the order) a single variable/feature in the training data.
  
  + Use the model to **_predict_** on the data with the permuted variable
  
  + See if there is a **_drop in predictive performance_**
  
  + **_Repeat_**
  

---

### The logic of permuting

Let's walk through the logic of why this would work using simulated data. 

```{r}
# Simulate some data 
set.seed(123)
N = 1000 # Number of observations
x1 <- rnorm(N) # independent variable 
x2 <- rnorm(N) # independent variable 
error <- rnorm(N) #  error
y <-  1 + 2*x1 + -2*x2 + error # dependent variable
D <- tibble(y,x1,x2) # Gather as data frame

head(D)
```


---

### The logic of permuting

```{r}
# Run Model
model <- lm(y ~ x1 + x2, data = D)

broom::tidy(model) %>% 
  mutate_if(is.numeric,function(x)  round(x,2))
```

---

### The logic of permuting


```{r,highlight=T}
# Randomly scamble the order of X1 and estimate OLS model
model <- 
  D %>% 
  mutate(x1 = sample(x1)) %>% #<<
  lm(y ~ x1 + x2, data = .)

broom::tidy(model) %>% 
  mutate_if(is.numeric,function(x)  round(x,2))
```

---

### The logic of permuting

![:space 5]

- Permuting a variable effectively **_breaks the statistical relationship_** between outcome and predictor.

- If a **_variable doesn't matter, then permuting it won't change the performance_** (as the model already doesn't rely on this variable )

- We must permute each variable **_multiple times_** as permuting is a random process 
  
  + We want to ensure a specific importance ordering isn't a results of a single permutation.

---

### Example

What features in the data best predict whether someone will vote or not?

```{r,echo=F}
set.seed(1234)
N = 1000
age <- round(runif(N,18,75))/100
political_therm <- round(runif(N,0,1),2)
visited_europe <- rbinom(N,1,.3)
eat_bread <- rbinom(N,1,.7)
z = -2+ 2*political_therm + 1.5*age + -.1*age^2 + 3*political_therm*age + -.01*visited_europe  
# z = -1+ 2*political_therm + 1.5*age + -.1*age^2 + .5*political_therm*age + -.5*visited_europe  
pr <- pnorm(z)
voted <- c("No","Yes")[rbinom(N,1,pr)+1]
vote_data <- tibble(voted,age,political_therm,visited_europe,eat_bread) %>% 
  mutate(voted = factor(voted,levels=c("Yes","No")))
```

```{r}
vote_data
```

---

### Example

Run Machine Learning Model.

```{r}
# Cross fold validation
set.seed(1988)
folds <- createFolds(vote_data$voted, k = 5)
control_conditions <- 
  trainControl(method='cv',
               summaryFunction = twoClassSummary,
               classProbs = TRUE,
               index = folds )

# Random Forest Model 
rf_model <-train(voted ~ ., data=vote_data,
                 method = "ranger", metric = "ROC",
                 trControl = control_conditions)

# Predictive accuracy
pred <- predict(rf_model,vote_data)
Metrics::accuracy(vote_data$voted,pred)
```

---

### Example

Feature importance.
```{r}
require(vip)
vi_permute(rf_model, # Machine learning model
           train = vote_data, # Training data 
           nsim = 10, # Number of times to permute each variable
           target = "voted", # outcome
           reference_class = "Yes", # what class are you predicting
           metric = "accuracy", # metric 
           pred_wrapper = predict) # prediction function
```

---

### Example

Use `vip` to auto generate a feature importance plot
```{r,fig.align="center",fig.height=3,fig.width=7,dpi=300}
vip(rf_model,train = vote_data,
    nsim = 10,method="permute",
    geom = "boxplot",target = "voted",
    reference_class = "Yes",metric = "accuracy",
    pred_wrapper = predict)
```

---

### Example 

We can use feature importance to **_select_** relevant variables, refining our models. 
```{r}
# Random Forest Model 
rf_model2 <-train(voted ~ age + political_therm, data=vote_data,
                 method = "ranger", metric = "ROC",
                 trControl = control_conditions)

# Predictive accuracy
pred <- predict(rf_model2,vote_data)
Metrics::accuracy(vote_data$voted,pred)
```



---

## Partial Dependence Plots (PDP)

![:space 5]

- Variable importance cannot tell us how variables **_relate_** to the outcome.

- Partial dependency plots show the **_marginal effect_** one or two features have on the predicted outcome of the model.

- A partial dependence plot can show whether the **_relationship_** between the target and a feature is linear, monotonic or more complex.

- The partial dependence plot is a **_global method_**: The method considers all instances and gives a statement about the global relationship of a feature with the predicted outcome.


---

## Partial Dependence Plots (PDP)

![:space 5]

- The steps:
  
  + Train a model
  
  + Identify the features that matter most (feature importance)
  
  + Manipulate the values of those features (one at a time) and take the average prediction, holding all other features at their observed values. 
  
  + Plot the values and interpret the curve. 


---

## Partial Dependence Plots (PDP)

Recall our model from before. Let's explore the marginal effect of age on the likelihood of voting. 

```{r,fig.align="center",fig.width=7,fig.height=3,dpi=300}
require(pdp)
partial(rf_model, pred.var = "age", plot = TRUE,prob=T,
        plot.engine = "ggplot2")
```

---

## Partial Dependence Plots (PDP)

Let's explore the marginal effect of age and political thermometer measure on the likelihood of voting. Useful for exploring **_interactions_** between data. 

```{r,eval=F,fig.align="center",fig.width=7,fig.height=3,dpi=300,highlight=T}
# Provide a grid of values to speed up computation time.
# more values, the longer the wait (like tuning)
grid_values <- #<<
  expand.grid(age = seq(.18,.75,by=.1), #<<
              political_therm = seq(0,1,by=.05)) #<<

partial(rf_model, 
        # Choose two predictive values
        pred.var = c("age", "political_therm"), #<<
        plot = TRUE, prob=T,
        #provide values to calc preds
        pred.grid = grid_values,  #<<
        # Plotting engine and color scheme
        plot.engine = "ggplot2",
        palette = "magma")
```

---

## Partial Dependence Plots (PDP)

Let's explore the marginal effect of age and political thermometer measure on the likelihood of voting. Useful for exploring **_interactions_** between data. 

```{r,echo=F,fig.align="center",fig.width=7,fig.height=3.5,dpi=300,highlight=T}
partial(rf_model, 
        # Choose two predicive values
        pred.var = c("age", "political_therm"), 
        plot = TRUE, prob=T,
        
        # Provide a grid of values to speed up computation time.
        pred.grid = expand.grid(age = seq(.18,.75,by=.1), #<<
                                political_therm = seq(.18,.75,by=.05)), #<<
        
        # Plotting engine and color scheme
        plot.engine = "ggplot2",
        palette = "magma")
```

---

##  Individual Conditional Expectation Plots (ICE)

- Partial dependency offers a plot of the **_average marginal effect_**; however, can obscure a heterogeneous relationship created by **_interactions_**. 

```{r,echo=F,fig.align="center",fig.width=10,fig.height=4.75,dpi=300}
set.seed(123)
N = 500
noise = .1
x1 <- rnorm(500)
y1 <- x1 + rnorm(N,0,noise)
y2 <- -1*x1 + rnorm(N,0,noise)
bind_rows(data.frame(y=y1,x=x1),data.frame(y=y2,x=x1)) %>% 
  ggplot(aes(x,y)) +
  geom_point(alpha=.5) +
  geom_smooth(se=F) +
  ggthemes::theme_fivethirtyeight() +
  theme(axis.title = element_text())
```

---

##  Individual Conditional Expectation Plots (ICE)

- Partial dependency offers a plot of the **_average marginal effect_**; however, can obscure a heterogeneous relationship created by **_interactions_**.

![:space 3]

- ICE plots plots show the **_marginal effect for each observation_** in the data. 

- We can observe if there are **_divergence_** or **_convergence_** in the predicted effect cross observations. 

- The PDP is just the average taken across the different ICE curves. 

---

##  Individual Conditional Expectation Plots (ICE)


```{r,fig.align="center",fig.width=7,fig.height=3,dpi=300,highlight=T}
partial(rf_model, 
        ice=T,alpha=.05, #<<
        pred.var = "age", plot = TRUE,prob=T,
        plot.engine = "ggplot2")
```

---

##  Individual Conditional Expectation Plots (ICE)


```{r,fig.align="center",fig.width=7,fig.height=3,dpi=300,highlight=T}
partial(rf_model, 
        ice=T,alpha=.05, center=T, #<<
        pred.var = "age", plot = TRUE,prob=T,
        plot.engine = "ggplot2")
```


---

## Global Surrogate Models

- Black box models (e.g. Random Forest) often do a better job at fitting the data but at the expense of interpretability. 

- A global surrogate model is an interpretable model (e.g. decision tree, linear model) trained to **_approximate the predictions of a black box model_**. 

- Steps:
  
  + Train a model
  
  + Get the predictions from that model.
  
  + Train an interpretable model (e.g. cart, lm) on those predictions, using the original training data or a subset of that data.
  
  + Examine the fit. If good (low-ish error), interpret the output. 

---

## Global Surrogate Models

```{r,fig.align="center",fig.width=9,fig.height=2.5,dpi=300,highlight=T}
# Extract the predictions from the black box model
y_probs <- predict(rf_model,vote_data,type = "prob")$Yes

# Generate a new data frame
vote_data2 <- vote_data %>% 
  mutate(y_probs=y_probs) %>% 
  select(-voted)

# Plot the predictions (just to see what these probabilities look like)
vote_data2 %>%
  ggplot(aes(y_probs)) +
  geom_histogram()
```

---

## Global Surrogate Models

```{r,fig.align="center",fig.width=9,fig.height=4,dpi=300,highlight=T}
# Train a Decision Tree surrogate model
require(rpart) # Let's use the model package directly (rather than in caret)
cart_surrogate <-rpart(y_probs ~ .,data = vote_data2,
                       control = rpart.control(maxdepth = 3))
rattle::fancyRpartPlot(cart_surrogate,sub = "",type = 1) # plot the tree
```


---

## Global Surrogate Models

- If the surrogate doesn't do a good job fitting the black box, then it's not useful for interpretation.

- Using R-squared, we can easily measure how good the surrogate model is in approximating the black box predictions.

```{r}
# Calculate the R squared
y_hat <- predict(cart_surrogate)
y <- vote_data2$y_probs
TSS = sum((y-mean(y))^2) # Total Sum of Squares
MSS = sum((y_hat-mean(y))^2) # Model Sum of Squares
R2 = MSS/TSS # R-Squared (variance explained)
round(R2,2)
```

- Ultimately, surrogate models are intuitive, model-agnostic ways of extracting substantive insights from a black box model. 

---

class: newsection

# Data Exploration in the Social Sciences

---

### Exploratory Data Analysis (EDA)

EDA is an **_iterative cycle_**:

![:space 5]

1. **_Generate_** questions about your data.

2. Search for answers by **_visualizing_**, **_transforming_**, and **_modelling_** your data.

3. Use what you learn to **_refine_** your questions and/or generate new questions.

![:space 5]

EDA is not a formal process with a strict set of rules. Feel free to investigate every idea that occurs to you. Some will work and offer valuable insights, others won't.

---

### Weak priors on the DGP

Often we are need to elicit answers from data where we know little about the **_data generating process_** (DGP)

  - Little to no academic work to guide our expectations.
  
  - New data type that does not conform cleanly to past operationalizations of well-defined theoretical concepts.
  
  - Data is entirely foreign to you 
      
      + Data outside your domain of _expertise_ --- e.g., you study conflict but you need to examine healthcare data.
      
      + Data outside your domain of _expectation_ --- e.g., a company's internal data logs that you need to draw insight on.

---

###  Data Exploration Steps 

Given new data, how best should one draw novel insights?

--

- **_![:text_color darkorange](Step 0)_**: **_Train/Test Split_**
  
  - Always the first step. 
  - Split the data and never look at, learn from, or examine the test data.
  - We can probe, model, and theorize about the training data _as long as we've held off test data_ to validate our conclusions. 

---

###  Data Exploration Steps 

Given new data, how best should one draw novel insights?

- **_![:text_color darkorange](Step 0)_**: **_Train/Test Split_**
- **_![:text_color darkorange](Step 1)_**: **_Understanding what data you have_**
  
  - <u>_Data Properties_</u>
      - What is the unit of analysis?
      - What are the data types?
          - integers, floats, categorical, ordered scales?
      - What sources of variation are there?
      - How is that data distributed?
      - Are there oddities in the data? 
          - e.g. housing price data with a lot of 0s. 
      - Is the data incomplete? If so, why?
      - Should the data be transformed?
      - Are there ways to back out new variables from the variables you have? ("feature engineering")
    
---

###  Data Exploration Steps 

Given new data, how best should one draw novel insights?

- **_![:text_color darkorange](Step 0)_**: **_Train/Test Split_**
- **_![:text_color darkorange](Step 1)_**: **_Understanding what data you have_**
  
  - <u>_Data Properties_</u>
  - <u>_Data Generation_</u> (questions to ask yourself)
      - How was the data generated?
          - Hand coded vs. machine coded?
          - Self-reported?
          - Convenience sample?
          - Complete? Incomplete? Why?
      - How fit is the data to answer your (or any) inquiry?
      - In what ways could the data be distorted or biased?
      - Are the concepts measured in the data clearly defined? 
          - If so, why do you believe that? 
          - If not, where does the issue lie?
    
---

###  Data Exploration Steps 

Given new data, how best should one draw novel insights?

- **_![:text_color darkorange](Step 0)_**: **_Train/Test Split_**
- **_![:text_color darkorange](Step 1)_**: **_Understanding what data you have_**
- **_![:text_color darkorange](Step 2)_**: **_Analyzing the relationships_**

  - What variables are highly correlated?
  - What is the relationship between the variables?
  - What can bi-variate models tell us about the relationship between two (or more) variables?
  - Does a decomposition reveal anything interesting about the variation in the data?
  - Does the data cluster in interesting ways? 
  - Can we plot the data spatially or temporally? Do any interesting patterns emerge? 
  
---

###  Data Exploration Steps 

Given new data, how best should one draw novel insights?

- **_![:text_color darkorange](Step 0)_**: **_Train/Test Split_**
- **_![:text_color darkorange](Step 1)_**: **_Understanding what data you have_**
- **_![:text_color darkorange](Step 2)_**: **_Analyzing the relationships_**
- **_![:text_color darkorange](Step 3)_**: **_Unpacking the outcome_**

  - What outcome variable do you care about (or seems important) in the data? 
  - Build a model that best predicts this outcome.
  - Explore the most predictive model
      - What features/variables mattered most in the prediction task?
      - What are the marginal effects of these most predictive features?
      - Is there any heterogeneity in the predicted value of the model across certain observations?

---

###  Data Exploration Steps 

Given new data, how best should one draw novel insights?

- **_![:text_color darkorange](Step 0)_**: **_Train/Test Split_**
- **_![:text_color darkorange](Step 1)_**: **_Understanding what data you have_**
- **_![:text_color darkorange](Step 2)_**: **_Analyzing the relationships_**
- **_![:text_color darkorange](Step 3)_**: **_Unpacking the outcome_**
- **_![:text_color darkorange](Step 4)_**: **_Theorize_**

  - What are some hypotheses that we might be able to generate from the data?
  - What confounders might be lurking generating spurious correlations?
	- What are some plausible interventions that we might be able to employ to test a causal relationship?
		- Are those interventions ethical?
		- Are those interventions practical?
		
---

###  Data Exploration Steps 

Given new data, how best should one draw novel insights?

- **_![:text_color darkorange](Step 0)_**: **_Train/Test Split_**
- **_![:text_color darkorange](Step 1)_**: **_Understanding what data you have_**
- **_![:text_color darkorange](Step 2)_**: **_Analyzing the relationships_**
- **_![:text_color darkorange](Step 3)_**: **_Unpacking the outcome_**
- **_![:text_color darkorange](Step 4)_**: **_Theorize_**


- **_![:text_color darkorange](Step 5)_**: **_Repeat 1 - 4_**

