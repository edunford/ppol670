<!DOCTYPE html>
<html>
  <head>
    <title> PPOL670 | Introduction to Data Science for Public Policy   Week 8       Application in Supervised Learning</title>
    <meta charset="utf-8">
    <meta name="author" content="  Prof. Eric Dunford  ◆  Georgetown University  ◆  McCourt School of Public Policy  ◆  eric.dunford@georgetown.edu" />
    <link rel="stylesheet" href="gu-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <font class = "title-panel"> PPOL670 | Introduction to Data Science for Public Policy </font> <font size=6, face="bold"> Week 8 </font> <br> <br> <font size=100, face="bold"> Application in Supervised Learning </font>
### <font class = "title-footer">  Prof. Eric Dunford  ◆  Georgetown University  ◆  McCourt School of Public Policy  ◆  <a href="mailto:eric.dunford@georgetown.edu" class="email">eric.dunford@georgetown.edu</a></font>

---




layout: true

&lt;div class="slide-footer"&gt;&lt;span&gt; 
PPOL670 | Introduction to Data Science for Public Policy

&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;

Week 8 &lt;!-- Week of the Footer Here --&gt;

&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;

Supervised Learning &lt;!-- Title of the lecture here --&gt;

&lt;/span&gt;&lt;/div&gt; 

---
class: outline

# Outline for Today 

&lt;br&gt;

- **Refresh on Classification Problems**

&lt;br&gt;

- **K Nearest Neighbors**

&lt;br&gt;

- **Classification and Regression Trees**

&lt;br&gt;

- **Bagging and Random Forests**


---

class: newsection

# Classification

---

&lt;br&gt;

.center[&lt;img src = "Figures/seperability.gif", width = 500&gt;]

---

### Decision Boundary 

.center[&lt;img src = "Figures/decision-boundary.png", width = 600&gt;]

---

### Conditional Probability 

Given `\(y \in 0,1\)` (two classes)

`$$pr(y_i = 1 | X = \vec{x_i})$$`

--

&lt;br&gt;


`$$\hat{f}(\vec{x_i}) \in [0,1]$$` 

where `\(\hat{f}(\cdot)\)` is an approximation for the true data generating process `\(f(\cdot)\)` and `\(\vec{x_i}\)` is a vector of variables for observation `\(i\)`.

--

&lt;br&gt; 

`$$\hat{y_i} =\begin{cases} 1~~\text{if}~~\hat{f}(\vec{x_i})  &gt; .5 \\ 0~~\text{if}~~\hat{f}(\vec{x_i})  \le .5 \end{cases}$$`

---

&lt;br&gt;&lt;br&gt;&lt;br&gt;

`$$\hat{y_i} =\begin{cases} 1~~\text{if}~~\hat{f}(\vec{x_i})  &gt; .5 \\ 0~~\text{if}~~\hat{f}(\vec{x_i})  \le .5 \end{cases}$$`

&lt;br&gt;

.center[.5 is an arbitrary threshold ( `\(\tau\)` ) that we can adjust. 

larger `\(\tau\)` means more false negatives

smaller `\(\tau\)` means more false positives
 ]


---

### Accuracy 


`$$\hat{y}^{test}_i = \hat{f}(x^{test}_i)$$`

.center[
| | `\(y^{test}_i\)` |  `\(\hat{y}^{test}_i\)` |  
|----|:------------:|:-------------------:|
| **True Positive** | 1 |  1 |  
| **False Positive** | 0 |  1 | 
| **False Negative** | 1 |  0 |  
| **True Negative** | 0 |  0 |  
]

--

&lt;br&gt;

$$ \text{average test error} = \frac{\sum^N_{i=1} I(y^{test}_i \ne \hat{y}^{test}_i)}{N}$$

---

### Accuracy

.center[
|                       |  `\(actual~(+)\)` |  `\(actual~(-)\)` |
|-----------------------|----------|----------|
| `\(Predicted~(+)\)`  |   True Positive (TP)       | False Positive (FP)          |
| `\(Predicted~(-)\)`  |   False Negative (FN)       |  True Negative (TN)         |

]

--

&lt;br&gt;

| Metric | Calculation |  Description |
|:---:|:-----:| -----|
| Accuracy | `\(\frac{TP + TN}{TP+FP+TN+FN}\)` | In total, how accurate is the model |
| Precision | `\(\frac{TP}{TP+FP}\)` | Of the true positives classified, how many are actually positive | 
| Specificity | `\(\frac{ TN }{ TN + FP }\)` | Of the actual true negatives, how many were correctly classified | 
| Recall/Sensitivity | `\(\frac{TP}{ TP + FN}\)` | Of the actual true positives, how many were correctly classified |

---

### ROC and AUC

.center[&lt;img src = "Figures/roc-plot.png", width = 600&gt;]

---

### Bias-Variance Tradeoff

&lt;br&gt;&lt;br&gt;
.center[&lt;img src = "Figures/bias-variance-tradeoff.png", width = 1300&gt;]


---

class: newsection

&lt;br&gt;

# `\(K\)`-Nearest Neighbors

---


&lt;img src="lecture-week-08-applications-supervised-ml-ppol670_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;


---

&lt;img src="lecture-week-08-applications-supervised-ml-ppol670_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="lecture-week-08-applications-supervised-ml-ppol670_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="lecture-week-08-applications-supervised-ml-ppol670_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;


---

&lt;img src="lecture-week-08-applications-supervised-ml-ppol670_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

### KNN

&lt;br&gt;

.center[&lt;img src =  "Figures/knn.png", width = 700&gt; ]

---

### KNN

- Non-parametric method that treats inputs as coordinate sets

- Classifies by distance of the new entry (test data) to existing entries (training data). 

--

- Distance can be conceptualized in a number ways. Euclidean distance is common:

`$$distance = \sqrt{(x_{ij} - x_{0j})^2}$$`

--

- Classification occurs as a "_majority vote_"

`$$Pr(y_{ik} = j~|~X = x_{ik}) = \frac{\sum^K_{k=1} I(y_{ik} =j )}{K}$$`

--

- Poor performance well in high dimensions


---

### `\(k\)` is a tuning parameter

&lt;br&gt;

.center[&lt;img src =  "Figures/low-high-k.png", width = 700&gt; ]


---

### `\(k\)` is a tuning parameter

.center[&lt;img src =  "Figures/knn-overfitting.png", width = 700&gt; ]


---

class: newsection

# Classification and Regression Trees

---


.center[&lt;img src =  "Figures/cart-full.png", width = 600&gt; ]

---

### Hitting Averages and Experience on Salary

&lt;br&gt;

.pull-left[
.center[&lt;img src =  "Figures/regression-tree-01.png", width = 700&gt; ]
]

.pull-right[
.center[&lt;img src =  "Figures/regression-tree-02.png", width = 700&gt; ]
]

---

&lt;br&gt;

The goal is to ﬁnd boxes ( `\(R_1 ,\dots , R_j\)` ) that minimize the RSS, given by

`$$\sum^J_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2$$`

where `\(\hat{y}_{R_j}\)` is the mean response for the training observations within the `\(j\)`th box.

--

We take a top-down, greedy approach that is known as **_recursive binary splitting_**

- **Top-down**: start with one region and break from there.

- **Greedy**: best split is made at each step (best split given the other splits that have been made)

---

### The essence of recursive binary splitting

- (1) From the predictors `\(X\)` select a predictor `\(X_j\)`

--

- (2) Find a cutpoint ( `\(s\)` ) that splits `\(X_j\)` into two regions that leads to the greatest possible reduction in RSS.

`$$R_1(j,s) = \{X~|~X_j &lt;s\}~\text{and}~ R_2 (j,s) = \{X~|~X_j \ge s\}$$` 

--

- (3) Looking for an `\(s\)` and `\(j\)` that minimizes

`$$\sum_{i: ~x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 + \sum_{i: ~x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2$$`

where `\(\hat{y}_{R_1}\)` and `\(\hat{y}_{R_2}\)` are the mean responses for the training data in region 1 ( `\(R_1(j,s)\)` ) and region 2  ( `\(R_2(j,s)\)` )

--

- (4) Repeat the process until a stopping criterion is met

---

### Tree Pruning


.center[&lt;img src =  "Figures/regression-tree-03.png", width = 600&gt; ]


---

### Tree Pruning

- Shallow trees (a few splits) can result in underfitting. 

- Deep trees (many splits) can result in overfitting

--

&lt;br&gt;

Balance by penalizing depth using a "**_complexity criterion_**" ( `\(\alpha\)` )

`$$\sum^T_{m=1}\sum_{i:x_i \in R_m} (y_i - \hat{y}_{R_m}) ^2 + \alpha T$$`

- `\(T\)` indicates the number of terminal nodes, 
- `\(R_m\)` is the rectangle (i.e. the subset of predictor space) corresponding to the `\(m\)`th terminal node
- `\(\hat{y}_{R_m}\)` is the predicted response associated with `\(R_m\)`.


---

### Tree Pruning as a tuning parameter



.pull-left[The tuning parameter `\(\alpha\)` controls a trade-off between the subtree’s complexity and its fit to the training data. 

&lt;br&gt;

- `\(\alpha \rightarrow 0\)` means deeper tree

- `\(\alpha \rightarrow 1\)` means a shallow tree

&lt;br&gt;

Need to use cross-validation to figure out the right value of `\(\alpha\)`.
]

.pull-right[

&lt;br&gt;&lt;br&gt;&lt;br&gt;
.center[&lt;img src =  "Figures/tree-complexity.png", width = 500, height =300&gt; ]

]

---

### Classification Trees

&lt;br&gt;

- Categorical rather than continuous outcome

- Similar process

- Predict most commonly occurring class of training observations in the region to which it belongs.

- Use the **_Gini Index_** as a measurement of error

`$$G = \sum^K_{k=1} \hat{p}_{mk} (1-\hat{p}_{mk})$$`

- Gini index gets small if all `\(\hat{p}_{mk}\)` are close to zero or one ("node purity")

---

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

.center[
&lt;img src =  "Figures/classification-tree-01.png", width = 1000&gt; 
]


---

### Regression vs. Trees

.center[
&lt;img src =  "Figures/reg-v-trees.png", width = 600&gt; 
]

---

### Pros and Cons of Trees

Pros:

- Easy to explain/visualize

- Easy handle qualitative predictors

- Can deal well with data inconsistencies

Cons:

- Less predictive accuracy

- Suffer from high variance


---

class: newsection

## Bagging &amp; Random Forest

---

### Bagging

**Bootstrap aggregation**, or **bagging**, is a general-purpose procedure for reducing the variance of a statistical learning method.

--

The idea:

- take many training sets from the data
- build separate tree one each training set 
- average the predictions

--

.center[
&lt;img src =  "Figures/bagging.png", width = 600&gt; 
]

---

### What is bootstrapping?


.center[
&lt;img src =  "Figures/bootstrap.png", width = 600&gt; 
]

---

Say we have two groups (people on a bus) and we want to know that their average age is statistically different. 


```r
bus1 &lt;- tibble(age=c(55,34,21,14,57,43,26))
bus2 &lt;- tibble(age=c(23,24,37,51,8,30,48))
```

--

We can just run a difference in means test.


```r
mean(bus1$age)-mean(bus2$age)
```

```
## [1] 4.142857
```


```r
t.test(bus1$age,bus2$age)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  bus1$age and bus2$age
## t = 0.48783, df = 11.885, p-value = 0.6345
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -14.38054  22.66625
## sample estimates:
## mean of x mean of y 
##  35.71429  31.57143
```


---

Say we have two groups (people on a bus) and we want to know that their average age is statistically different. 


```r
bus1 &lt;- tibble(age=c(55,34,21,14,57,43,26))
bus2 &lt;- tibble(age=c(23,24,37,51,8,30,48))
```

Or we could bootstrap. The key is sampling _with replacement_.

.pull-left[

```r
bus1 %&gt;% 
  sample_n(4,replace = 5)
```

```
## # A tibble: 4 x 1
##     age
##   &lt;dbl&gt;
## 1    14
## 2    34
## 3    55
## 4    43
```
]
.pull-right[

```r
bus2 %&gt;% 
  sample_n(4,replace = 5)
```

```
## # A tibble: 4 x 1
##     age
##   &lt;dbl&gt;
## 1    24
## 2    48
## 3     8
## 4    48
```
]

---

Do this many times...


```r
boot_samples = rep(0,100)
for(i in 1:100){
  boot_samp1 = bus1 %&gt;% sample_n(4,replace = 5)
  boot_samp2 = bus2 %&gt;% sample_n(4,replace = 5)
  mu1 &lt;- mean(boot_samp1$age)
  mu2 &lt;- mean(boot_samp2$age)
  boot_samples[i] &lt;- mu1 - mu2
}
tibble(means = boot_samples) %&gt;% 
ggplot(aes(x=means)) + 
  geom_density(fill="grey30") + 
* geom_vline(xintercept = 4.14,color="red") # actual mean
```

&lt;img src="lecture-week-08-applications-supervised-ml-ppol670_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;


---

### Same idea but with trees... 

Grow many of trees then average the predictions


`$$\hat{f}_{avg}(x) = \frac{\sum^B_{b=1} \hat{f}_b(x) }{B}$$`

&lt;br&gt;

--

.center[
&lt;img src =  "Figures/bagging-in-practice.png", width = 800&gt; 
]

---

### Random Forest 

- Issue with bagging is that the trees are **_highly correlated_**. 

--

- One way around this is to also take a **_random sample of predictors_** at each split (in addition to bagging). Algorithm is _not allowed_ to consider a majority of the available predictors.

--

- Number of predictors the algorithm is able to select  (`mtry`) is a **_tuning parameter_**

.center[
&lt;img src =  "Figures/random-forest.png", width = 300&gt; 
]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
