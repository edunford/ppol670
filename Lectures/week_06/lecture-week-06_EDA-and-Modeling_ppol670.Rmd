---
title: 
    <font class = "title-panel"> PPOL670 | Introduction to Data Science for Public Policy  </font>
  <font size=6, face="bold"> Week 6 </font> 
  <br>
  <br>
  <font size=100, face="bold"> Exploratory Data Anlaysis and Modeling </font>
author: 
  <font class = "title-footer"> 
  &emsp;Prof. Eric Dunford &emsp;&#9670;&emsp; Georgetown University &emsp;&#9670;&emsp; McCourt School of Public Policy &emsp;&#9670;&emsp; eric.dunford@georgetown.edu</font>
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: "gu-theme.css"
    nature:
      highlightStyle: github
      countIncrementalSlides: False
      highlightLines: true
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,message=F,error=F,warning = F,cache=T)
require(tidyverse)
require(recipes)
```

layout: true

<div class="slide-footer"><span> 
PPOL670 | Introduction to Data Science for Public Policy

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;

Week 6 <!-- Week of the Footer Here -->

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;

EDA and Modeling <!-- Title of the lecture here -->

</span></div> 

---
class: outline

# Outline for Today 

- Briefly discuss expectations regarding **Memo 2**

- Delve into **Exploratory Data Analysis** techniques

- linear **modeling**

- Talk about **Pre-processing data** 

---

# General

- Do the readings

  + Specifically the readings on RMarkdown (week 2)

<br>

- "Diligent De-Bugging" Clause 
  
  + Google before you ask
  
  + Prove to us what you've done.
  
<br>

- Reproducibility and Transparency: more than just concepts. They impact your grade.



---

# Thoughts on Memo 1

- "Traditional"
  
  + Statistical analysis (e.g. diff-in-diff)
  
  + Using traditional sources (e.g. survey data)
  
--

- Careful! This is not a stats course. This is a data science course. Make sure to use what we've learned in class on your projects.

--

- More precise discussion about data 

- "Potential Issues" need to actually be problems. 

---

#  Memo 2

For Memo 2, please:

- Restate the problem and your hypothesis/expectations for the project

- Outline what you plan to do: visualizations, predictive analysis, interactive applications, text as data, 

- Provide a description of the analysis and tools your group plans on using: 

- **Key question to ask yourself**: are we employing concepts we learned in _this_ course? If the answer is "no", then back to the drawing board. 

---

class: newsection

# Exploratory Data Analysis

---

### Exploratory Data Analysis (EDA)

EDA is an **iterative cycle**:

1. **Generate** questions about your data.

2. Search for answers by **visualising**, **transforming**, and **modelling** your data.

3. Use what you learn to **refine** your questions and/or generate new questions.

EDA is not a formal process with a strict set of rules, it's a state of mind. Feel free to investigate every idea that occurs to you. Some will work and offer valuable insights, others won't

---

### Asking Questions

- EDA is fundamentally a creative process. 

- The key to asking _quality_ questions is to generate a large _quantity_ of questions.

--

<br><br>

There is no rule about which questions to ask, but here are two important ones you should always ask: 

1. What type of **variation** occurs within my variables?

2. What type of **covariation** occurs between my variables?

---

### Data

To give you a feel for the EDA process, I'll look through new (to me) data on crimes committed in Chicago from 2012 to 2016. 

These data can be downloaded from the [`crimedata` package](https://cran.r-project.org/web/packages/crimedata/crimedata.pdf), which provides an `R` wrapper for the FBI Crime data API that we encountered last week. 

```{r,eval=F,highlight=TRUE}
crimes = crimedata::get_crime_data(years=2012:2016,
                                   cities="Chicago",
                                   type = "sample") #<< 
```

More information on these data can be found [here](https://osf.io/zyaqn/).

---

```{r}
crimes = read_csv("Data/chicago-crime-data.csv")
glimpse(crimes)
```

---

class:shrink

### Distribution

```{r}
crimes %>% select(offense_code:location_category) %>% skimr::skim(.) 
```

---

### Variation

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  mutate(location_category = fct_infreq(location_category)) %>% 
  ggplot(aes(location_category)) + 
  geom_bar() +
  coord_flip()
```

---

### Variation

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  mutate(offense_group = fct_infreq(offense_group)) %>% 
  ggplot(aes(offense_group)) + 
  geom_bar() +
  coord_flip()
```

---

### Variation

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  mutate(offense_against = fct_infreq(offense_against)) %>% 
  ggplot(aes(offense_against)) + 
  geom_bar() +
  coord_flip()
```

---

### Spatial Variation

```{r,fig.align="center",fig.height=5}
crimes %>% 
  ggplot(aes(longitude,latitude)) +
  geom_point(alpha=.1,color="darkred") +
  theme_minimal()
```

---

### Spatial Variation

```{r,fig.align="center",fig.height=5}
crimes %>% 
  ggplot(aes(longitude,latitude)) +
  geom_point(alpha=.1,color="darkred") +
  geom_density_2d() +
  theme_minimal() 
```

---

```{r,fig.align="center",fig.height=5}
crimes %>% 
  ggplot(aes(longitude,latitude)) +
  stat_density_2d(aes(fill = stat(level)), geom = "polygon") +
  scale_fill_viridis_c() +
  theme_minimal() 
```

---

### Temporal Variation


```{r,fig.align="center",fig.height=5,fig.width=9}
crimes <- crimes %>% mutate(date=lubridate::ymd_hms(date_single)) 

crimes %>% 
  ggplot(aes(date)) +
  geom_histogram(color="white",bins=50,fill="black") +
  theme_minimal()
```

---

```{r}
crimes <- 
  crimes %>% 
  mutate(month = lubridate::round_date(date,unit = 'month')) 

crimes %>% 
  group_by(month) %>% 
  count()
```

---

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  group_by(month) %>% 
  count() %>% 
  ggplot(aes(month,n)) +
  geom_line() + geom_point() +
  ylim(0,400)
```

---

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  group_by(month) %>% 
  count() %>% 
  ggplot(aes(month,n)) +
  geom_line() + geom_point() +
  ylim(0,400) +
  geom_smooth(method="lm")
```

---

```{r,highlight=T,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  group_by(month) %>% 
  count() %>% 
  ggplot(aes(month,n)) +
  geom_line() + geom_point() +
  ylim(0,400) +
  geom_smooth(method="loess") #<<
```

---

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  mutate(date = lubridate::round_date(date,unit="day")) %>% 
  group_by(date) %>% 
  count() %>% 
  ggplot(aes(date,n)) +
  geom_line(alpha=.1) + geom_point(alpha=.7) +
  geom_smooth(method="loess")
```

---

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  mutate(date = lubridate::year(date)) %>% 
  ggplot(aes(date)) +
  geom_bar(fill="steelblue")
```

---

```{r,highlight=T,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  group_by(month,location_category) %>% 
  count() %>% 
  ggplot(aes(month,n,color=location_category)) +
  geom_line() + geom_point() +
  gghighlight::gghighlight(max(n)>30) + #<<
  theme(legend.position="none")
```

---

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  filter(location_category %in% c("street","residence")) %>% 
   ggplot(aes(longitude,latitude,color = location_category)) +
  geom_point(alpha=.5) +
  ggthemes::theme_map() 
```

---

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  filter(location_category %in% c("street","residence")) %>% 
   ggplot(aes(longitude,latitude,color = location_category)) +
  geom_point(alpha=.25) +
  ggthemes::theme_map() +
  facet_wrap(~location_category)
```

---

```{r,highlight=T,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  group_by(month,offense_group) %>% 
  count() %>% 
  ggplot(aes(month,n,color=offense_group)) +
  geom_line() + geom_point() +
  gghighlight::gghighlight(max(n)>40) + #<<
  theme(legend.position="none")
```

---

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  mutate(month_num = lubridate::month(date)) %>% 
  group_by(month_num,offense_group) %>% 
  count() %>% 
  filter(max(n)>=50) %>% 
  ggplot(aes(factor(month_num),offense_group,fill=n)) +
  geom_tile() +
  scale_fill_viridis_c()
```

---

```{r,fig.align="center",fig.height=5,fig.width=9}
crimes %>% 
  mutate(month_num = lubridate::month(date)) %>% 
  group_by(month_num,location_category) %>% 
  count() %>% 
  filter(max(n)>=50) %>% 
  ggplot(aes(factor(month_num),location_category,fill=n)) +
  geom_tile() +
  scale_fill_viridis_c()
```


---

```{r,fig.align="center",fig.width=10,fig.height=5}
crimes %>% 
  group_by(offense_group,location_category) %>% 
  count() %>% 
  filter(max(n) > 100) %>% 
  ggplot(aes(location_category,offense_group,fill=log(n))) +
  geom_tile() +
  scale_fill_viridis_c() 
```

---

```{r,fig.align="center",fig.width=10,fig.height=5}
crimes %>% 
  group_by(offense_group,offense_against) %>% 
  count() %>% 
  filter(max(n) > 50) %>%
  ggplot(aes(offense_against,offense_group,fill=log(n))) +
  geom_tile() +
  scale_fill_viridis_c() 
```

---

### Missingness

```{r}
crimes %>% 
  summarize_all(function(x) sum(is.na(x))) %>% 
  glimpse
```

---

### EDA... always to be continued

This was just an initial probe. There are many more ways we could try and slide the data to look at potential relationships that might offer some insights. 

--

The point is: 

- Exploration takes **time**

- There is no **roadmap** (else why would we need to explore?)

- EDA is the most **valuable** use of your time as a data scientist. 
  
  - Knowing your data requires that you to delve into it. 
  
  - Delving into it requires that you ask questions of the data.
  
  - Questions always lead to more (usually more interesting) questions

---

### EDA Checklist

- Where are the sources of variation?

- Which variables covary/correlate?

- What's the distribution across groupings (time, space, organization, etc.)?

- Are there outliers? Why? Where are they? 

- Is there missing data? If so, to what extent? For what variables? Is there something systematic about the missingness?

- Is there clustering in the data? (we'll delve more into this...)

- What does the data look like at different units of analysis (e.g. day, month, year)?

- etc.

---

class:newsection

# Modeling

---

### Linear Modeling 

The goal of a model is to provide a simple low-dimensional summary of a data set. 

<br>

Modeling is a form of **dimension reduction**: many data values are compressed into a single numerical statement.

<br>

Most all of you have completed the McCourt stats sequence, so we won't focus too much on statistical theory. Rather the aim will be to show how we can use modeling as exploratory tool. 


---

### Data 

```{r}
require(gapminder) # gapminder data 
gapminder %>% 
  glimpse()
```

---

### Economic Development on Life expectancy

```{r,fig.align="center",fig.height=5,fig.width=9}
gapminder %>% 
  ggplot(aes(gdpPercap,lifeExp)) +
  geom_point() +
  geom_smooth(method="lm") +
  geom_smooth(method="loess",color="orange")
```

---

### Economic Development on Life expectancy

```{r}
gapminder %>% 
  lm(lifeExp ~ gdpPercap,data=.) 
```


---

### Economic Development on Life expectancy

```{r,fig.align="center",fig.height=3.75,fig.width=9}
gapminder <- 
  gapminder %>% 
  mutate(ln_gdpPercap = log(gdpPercap),
         ln_pop = log(pop)) 

gapminder %>% 
  ggplot(aes(ln_gdpPercap,lifeExp)) +
  geom_point() +
  geom_smooth(method="lm")
```

---

```{r}
gapminder %>% 
  lm(lifeExp ~ ln_gdpPercap,data=.) 
```

---


```{r,highlight=T}
gapminder %>% 
  lm(lifeExp ~ ln_gdpPercap,data=.) %>% 
  summary(.) #<<
```

---


```{r,highlight=T}
gapminder %>% 
  lm(lifeExp ~ ln_gdpPercap,data=.) %>% 
  broom::tidy(.) #<<
```

---


Do effects differ by **year**?

```{r,fig.align="center",fig.height=4,fig.width=9}
gapminder %>% 
  ggplot(aes(ln_gdpPercap,lifeExp,color=factor(year))) +
  geom_point() +
  geom_smooth(method="lm",se=F) +
  theme(legend.position = "none")
```

---

Do effects differ by **country**?

```{r,fig.align="center",fig.height=4,fig.width=9}
gapminder %>% 
  ggplot(aes(ln_gdpPercap,lifeExp,color=country)) +
  geom_point() +
  geom_smooth(method="lm",se=F) +
  theme(legend.position = "none")
```

---

### Many models

Do effects differ by **country**?

```{r}
my_model <- function(df) coef(lm(lifeExp ~ ln_gdpPercap,data=df))[2]
gapminder %>% 
  filter(country=="Nigeria") %>% 
  my_model(.) 
```

---

### Many models

```{r}
countries = unique(gapminder$country)
country_models <- c()
for(c in countries){
  mod_results <- 
    gapminder %>% 
    filter(country==c) %>% 
    my_model(.) %>% 
    tibble(estimate=.) %>% 
    mutate(country=c)
  country_models <- bind_rows(country_models,mod_results)
}
head(country_models)
```

---

### Many models

```{r,fig.align="center",fig.height=5,fig.width=9}
country_models %>% 
  mutate(effect = ifelse(estimate>=0,"positive","negative")) %>% 
  ggplot(aes(x=estimate,fill=effect)) +
  geom_histogram(color="white",bins=75) +
  scale_fill_manual(values = c("darkred","dodgerblue3"))
```


---

`modelr` useful package for extracting model features (like residuals or predictions)

```{r, highlight=T}
require(modelr) #<<
mod <- lm(lifeExp ~ ln_gdpPercap,data=gapminder)


gapminder %>% 
  modelr::add_residuals(mod) %>%  # Adds a column of residuals #<<
  glimpse()
```

---

What does this tell us?

```{r,fig.align="center",fig.height=5,fig.width=9}
gapminder %>% 
  modelr::add_residuals(mod) %>%  
  ggplot(aes(year,resid,group=country)) +
  geom_line(alpha=.5)
```



---

```{r,fig.align="center",fig.height=5,fig.width=9}
gapminder %>% 
  modelr::add_residuals(mod) %>%  
  group_by(country) %>% 
  ggplot(aes(year,resid,color=country)) +
  geom_line() + geom_point() +
  gghighlight::gghighlight(resid<=-20 | resid >= 15,
                           use_group_by = TRUE)
```

---

What does this tell us?

```{r,fig.align="center",fig.height=5,fig.width=9}
gapminder %>% 
  modelr::add_residuals(mod) %>%  
  ggplot(aes(year,resid,group=country)) +
  geom_line(alpha=.5) +
  facet_wrap(~continent)
```

---

### Model objects

```{r}
mod <- lm(lifeExp ~ ln_gdpPercap,data=gapminder)
names(mod)
```

```{r}
mod$coefficients
```

```{r}
mod$model # Data 
```


---

### Presenting Models

```{r,result="asis"}
stargazer::stargazer(mod,type = "text")
```

---

### Presenting Models

`stargazer` offers a number of different formats for presenting model output.

```{r,eval=F}
stargazer::stargazer(mod,type = "text")

stargazer::stargazer(mod,type = "latex")

stargazer::stargazer(mod,type = "html")
```



---

class:newsection

# Pre-Processing Data

---

### Feature Cleaning 

We've already talked about **data manipulation**.

- raw data to **tidy** data
- transform the **unit of analysis**
- **class** management (e.g. characters to dates)

--

<br>

However, often our variables exist on different scales, which can complicate machine learning and statistics tasks.

By complicate, I mean it can make **optimization problems intractable**. 

---

### Feature Cleaning 

Feature (or variable) cleaning it the process of curating a **design matrix** for a machine learning or modeling task. 

<br>

> In statistics, a **design matrix** (also known as regressor matrix or model matrix) is a matrix of values of explanatory variables of a set of objects, often denoted by X. Each row represents an individual object, with the successive columns corresponding to the variables and their specific values for that object.

<br>

This is known as data **pre-processing** the data.

---

### Feature Cleaning 

```{r,echo=F}
set.seed(123)
N = 1000
x <- rnorm(N,5,5)
y = 1+ 2*x + rnorm(N,0,5) 
y = scale(y)[,1] #y/(max(y)-min(y))
x = x * 100000
D = tibble(y,x) 
```

```{r,fig.align="center",fig.width=10,fig.height=6}
D %>% ggplot(aes(x,y)) + geom_point()
```


---

### Feature Cleaning 


```{r,fig.align="center",fig.width=10,fig.height=5}
D %>% 
  gather(var,val) %>% 
  ggplot(aes(val,fill=var)) + geom_density()
```

---

### Feature Cleaning 

Variables at different scales can impact estimation. The coefficient estimates are scaled down (i.e. a unit change in x has a really really small unit change in y). 

```{r}
lm(y~x,data=D) %>% 
  coef(.) %>% 
  round(.,6)
```


---

### Scaling

**scaling** is the process of transforming our data so that it all falls within the same numerical range. Below `x` is transformed to have a mean of `0` and a variance of `1`.

```{r,fig.align="center",fig.width=10,fig.height=3.5}
D %>% 
  mutate(x = scale(x)) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,fill=var)) +geom_density(alpha=.5)
```

---

### Scaling

**scaling** is the process of transforming our data so that it all falls within the same numerical range. Below `x` is transformed to have a mean of `0` and a variance of `1`.

```{r,fig.align="center",fig.width=10,fig.height=3.5,highlight=T}
D %>% 
  mutate(x = (x-mean(x))/sd(x) ) %>% #<<
  gather(var,val) %>% 
  ggplot(aes(val,fill=var)) +geom_density(alpha=.5)
```

---

### Scaling

The scaled versions of our variables behave better. 

```{r}
D %>% 
  mutate(x = (x-mean(x))/sd(x) ) %>%
  lm(y~x,data=.) %>% 
  coef(.) %>% 
  round(.,3)
```

---

### Data preprocessing

<br><br>

Common pre-processing tasks:

- **Scaling** and transforming continuous values 

- Converting categorical variables to **dummy** variables.

- Detecting and **imputing** missing values


---

### `recipes()` package


.pull-left[
<br><br>
.center[<img src="Figures/recipes_hex_thumb.png",width=700>]
]

.pull-right[
[`recipes`](https://tidymodels.github.io/recipes/) package is an alternative method for creating and preprocessing design matrices that can be used for modeling or visualization.


The idea of the `recipes` package is to define a recipe or blueprint that can be used to sequentially define the encodings and preprocessing of the data (i.e. “feature engineering”).
]


---

### `recipes()` package

.pull-left[
<br><br>
.center[<img src="Figures/recipes_hex_thumb.png">]
]
.pull-right[
The basic setup of `recipes()`:

- Initialize a recipe object

- Specify the transformation steps

- Estimates the required quantities and statistics required by any operations.

- Apply the transformations 
]

---

### Data 

Data on whether a person will pay back a bank loan. The outcome variable is `status`. 13 other variables track features about the debtor and the loan. (See `?credit_data` for more details.)

```{r}
data("credit_data") # Load data (from recipes package)
glimpse(credit_data)
```

---

Variables of different types

```{r}
credit_data %>% 
  summarize_all(class) %>% 
  glimpse()
```

---

Variables on different scales... with some missing values!

```{r,highlight=T}
credit_data %>% 
  summarize_if(is.numeric, function(x) mean(x)) %>% #<<
  glimpse()
```

---

Variables on different scales... 

```{r}
credit_data %>% 
  summarize_if(is.numeric, function(x) mean(x,na.rm = T)) %>% 
  glimpse()
```

---

Categorical variables...

```{r}
credit_data %>% 
  select_if(is.factor) %>% 
  glimpse()
```

```{r}
credit_data %>% 
  select_if(is.factor) %>% 
  summarize_all(function(x) sum(is.na(x)))
```

---

### Building a recipe

The package operates by laying out a series of steps that are then itemized. Once we have all our steps in place we then `bake` the recipe (i.e. execute and transform the data all at once).

<br> 

`step_`s we need to perform:

1. imput any missing values.

2. scale the continuous variables

3. convert the categorical variables to dummy variables


---

### Building a recipe

First, let's initialize the recipe object. 

```{r}
our_recipe <- recipe(Status ~ ., data = credit_data)
our_recipe
```

---

### (1) Imput any missing values

`recipes` offers many different forms of imputation.

<br> 

.center[
```{r,echo=F,fig.align="center"}
grep("impute$", ls("package:recipes"), value = TRUE) %>% 
  tibble(`Imputation Methods` = .) %>% 
  kableExtra::kable(.,align="left") %>% 
  kableExtra::kable_styling(position = "center")
```
]

---

### (1) Imput any missing values

`recipes` offers many different forms of imputation.

```{r}
our_recipe <-
  our_recipe %>% 
  step_knnimpute(all_predictors())
our_recipe
```

---

### (2) Scale the continuous variables

```{r}
our_recipe <-
  our_recipe %>% 
  step_center(all_numeric()) %>% # Center mean around 0
  step_scale(all_numeric()) # Set variance to 1
our_recipe
```

---

### (3) Convert the categories to dummies

```{r}
our_recipe <- 
  our_recipe %>% 
  step_dummy(all_nominal())
our_recipe
```

---

### Prepare the recipe

<!-- Need to calculate all the necessary statistics and values for the transformations.  -->

```{r}
prepared_recipe <- our_recipe %>% prep()
prepared_recipe
```


---

### Bake!

<!-- Now that the recipe is prepared... we can apply it to our data. -->

```{r}
glimpse(credit_data) # Before
```

---

### Bake!

```{r}
dat_processed <- bake(prepared_recipe,new_data = credit_data)
glimpse(dat_processed) # After
```

---

<br>
.pull-left[
```{r,fig.align="center"}
credit_data %>% 
  ggplot(aes(Seniority)) +
  geom_density(fill="pink",
               alpha=.5)
```
]

.pull-right[
```{r,fig.align="center"}
dat_processed %>% 
  ggplot(aes(Seniority)) +
  geom_density(fill="pink",
               alpha=.5)
```
]


---

### `recipe()`

<br>
`recipe()` provides a way to systematically transform our data and apply it to any _new_ versions of the data.

<br>

This becomes really important when pre-processing **training data** that we then need to apply to **test data** in order to calculate our out of sample predictions (more on this later!)

<br> 

The `prep()` function allows us to use the same statistics (like the mean when centering) that we used to process the old and new data. 

Then `bake()` allows allows us to seemlessly and implement those steps. 


